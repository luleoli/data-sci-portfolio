{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57283d8d",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "# Introduction to Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24febb0",
   "metadata": {},
   "source": [
    "### Regression with an Abalone Dataset\n",
    "\n",
    "Playground Series - Season 4, Episode 4\n",
    "\n",
    "https://www.kaggle.com/competitions/playground-series-s4e4/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a68b1",
   "metadata": {},
   "source": [
    "Simplified EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f22a39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 127.98it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 75.41it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df = pd.read_csv(\"./data/playground-series-s4e4/train.csv\")\n",
    "train_report = ProfileReport(train_df, title=\"Train\", progress_bar=False)\n",
    "train_report.to_file(\"./profile/abalone.html\")\n",
    "\n",
    "test_df = pd.read_csv(\"./data/playground-series-s4e4/test.csv\")\n",
    "test_report = ProfileReport(test_df, title=\"Test\", progress_bar=False)\n",
    "\n",
    "comparison_report = train_report.compare(test_report)\n",
    "comparison_report.to_file(\"abalone_comp.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear Regression\n",
    "\n",
    "What is the problem ?\n",
    "\n",
    "Ways to solve it ?\n",
    "\n",
    "How to measure if where are efficient ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d178ec",
   "metadata": {},
   "source": [
    "Linear Regression (here we go...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276946ad",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "The loss function used is the **Mean Squared Error (MSE)**. This is implicitly calculated during the gradient descent process.\n",
    "\n",
    "The gradients $dw$ and $db$ are derived from the partial derivatives of the MSE loss function with respect to the weights and bias, respectively: \n",
    "\n",
    "\\begin{align}\n",
    "(\\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2) \n",
    "\\end{align}\n",
    "- **Gradients**:\n",
    "\n",
    "\\begin{align}\n",
    "(dw = \\frac{1}{n} X^T (\\hat{y} - y))\n",
    "\\end{align}  \n",
    "\n",
    "\\begin{align}\n",
    "(db = \\frac{1}{n} \\sum (\\hat{y} - y))\n",
    "\\end{align}\n",
    "\n",
    "Here, \\(\\hat{y}\\) is the predicted value: \\(\\hat{y} = X \\cdot \\text{weights} + \\text{bias}\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e47bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Linear Regression using Gradient Descent\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize weights and bias\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient Descent\n",
    "        for _ in range(self.epochs):\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
